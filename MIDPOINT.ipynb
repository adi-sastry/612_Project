{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749bd115",
   "metadata": {},
   "source": [
    "# Forecasting Air Pollution with Informer\n",
    "\n",
    "*MSML 612 - Interim Project*  \n",
    "Rohan, Shubhang, Adi, Swati, Josh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, datetime, math, json, random\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# PyTorch stack\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_forecasting import (\n",
    "    TimeSeriesDataSet, TemporalFusionTransformer, Baseline,\n",
    "    QuantileLoss, NaNLabelEncoder,\n",
    ")\n",
    "# NeuralForecast Informer\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import Informer as NFInformer\n",
    "from neuralforecast.losses.pytorch import MSE     # loss instance\n",
    "pl.seed_everything(42, workers=True)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e36396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper‑parameters and file locations \n",
    "CONFIG = dict(\n",
    "    RAW_FILE='pollution_2000_2023.csv',\n",
    "    SAVE_DIR='data/curated', # where processed files will go\n",
    "    TRAIN_END='2016-12-31',  # end of training period\n",
    "    VAL_END='2020-12-31', # end of validation period\n",
    "    INPUT_WINDOW=30, # how many past days the model sees\n",
    "    PRED_WINDOW=3, # how many future days it predicts\n",
    "    BATCH_SIZE=128,\n",
    "    MAX_EPOCHS=30, # training epochs for Lightning baselines\n",
    ")\n",
    "# making sure SAVE_DIR exists\n",
    "Path(CONFIG['SAVE_DIR']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2200761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and formatting raw data\n",
    "def load_raw(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    # snake_case all column names\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "                  .str.replace(' ', '_')\n",
    "                  .str.lower()\n",
    "    )\n",
    "    # ensure 'date' is datetime64[ns]\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "#loading the file\n",
    "df_raw = load_raw(CONFIG['RAW_FILE'])\n",
    "print(f'Raw rows loaded: {len(df_raw):,}')\n",
    "# Peek at 3 random rows (transposing for readability)\n",
    "display(df_raw.sample(3).T.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc33c1",
   "metadata": {},
   "source": [
    "# 1. ETL - reshape to NeuralForecast Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c88377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only necessary columns\n",
    "value_cols = ['o3_mean', 'co_mean', 'so2_mean', 'no2_mean']\n",
    "id_cols = ['date', 'state', 'city']\n",
    "df_nf = df_raw[id_cols + value_cols].dropna().copy()\n",
    "\n",
    "#melting to transform to long format\n",
    "#each row becomes (date, city, state, pollutant, value)\n",
    "df_nf = df_nf.melt(\n",
    "    id_vars=['date', 'state', 'city'],\n",
    "    value_vars=value_cols,\n",
    "    var_name='pollutant',\n",
    "    value_name='y'\n",
    ")\n",
    "# building unique_id  (ie 'denver_colorado_o3')\n",
    "df_nf['unique_id'] = (\n",
    "    df_nf['city'].str.replace(' ', '')\n",
    "    + '_' + df_nf['state'].str.replace(' ', '')\n",
    "    + '_' + df_nf['pollutant']\n",
    ").str.lower()\n",
    "\n",
    "#renaming nf convention\n",
    "df_nf = df_nf.rename(columns={'date': 'ds'})[['unique_id', 'ds', 'y']]\n",
    "\n",
    "print('long‑format preview:')\n",
    "display(df_nf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029562b",
   "metadata": {},
   "source": [
    "## 1.2 Train / Val / Test split + standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77854e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting cut‑off dates to pandas Timestamps\n",
    "train_cut = pd.Timestamp(CONFIG['TRAIN_END'])\n",
    "val_cut = pd.Timestamp(CONFIG['VAL_END'])\n",
    "\n",
    "# masks for each period\n",
    "train_mask = df_nf['ds'] <= train_cut\n",
    "val_mask = (df_nf['ds'] > train_cut) & (df_nf['ds'] <= val_cut)\n",
    "test_mask = df_nf['ds'] > val_cut\n",
    "\n",
    "train_df = df_nf[train_mask].copy()\n",
    "val_df = df_nf[val_mask].copy()\n",
    "test_df = df_nf[test_mask].copy()\n",
    "\n",
    "print('Rows [train / val / test] =', [len(train_df), len(val_df), len(test_df)])\n",
    "#standardizing target using train statistics\n",
    "\n",
    "scaler = StandardScaler().fit(train_df[['y']])\n",
    "\n",
    "train_df['y'] = scaler.transform(train_df[['y']])\n",
    "val_df['y'] = scaler.transform(val_df[['y']])\n",
    "test_df['y'] = scaler.transform(test_df[['y']])\n",
    "\n",
    "# saving the processed parquet files for reproducibility\n",
    "Path('data/neuralforecast').mkdir(parents=True, exist_ok=True)\n",
    "train_df.to_parquet('data/neuralforecast/train.parquet')\n",
    "val_df.to_parquet('data/neuralforecast/val.parquet')\n",
    "test_df.to_parquet('data/neuralforecast/test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a0e84",
   "metadata": {},
   "source": [
    "# 2. Train the Informer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b25e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train + val (NF handles early stopping) \n",
    "df_train_val = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "#instantiating transformer\n",
    "nf_informer = NFInformer(\n",
    "    h = CONFIG['PRED_WINDOW'],      # forecast horizon  (required)\n",
    "    input_size = CONFIG['INPUT_WINDOW'],     # look‑back length  (required)\n",
    "    hidden_size = 64,# embedding dimension (was d_model)\n",
    "    n_head = 4, # multi‑head attention\n",
    "    factor = 5,  # ProbSparse top‑k factor\n",
    "    dropout = 0.1, # regularization\n",
    "    learning_rate = 1e-3, # optimizer LR\n",
    "    loss = MSE(), # pass a *loss instance*\n",
    "    batch_size = 64, \n",
    "    max_steps = 1000,# training cap\n",
    ")\n",
    "\n",
    "# wrapping inside NeuralForecast orchestrator\n",
    "nf = NeuralForecast(models=[nf_informer], freq='D')\n",
    "\n",
    "# fit\n",
    "print('Training Informer …')\n",
    "nf.fit(df=df_train_val)   # internally uses early stopping\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8242c5d",
   "metadata": {},
   "source": [
    "# 3. Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating forecasts\n",
    "print(\"Generating 3‑day forecasts …\")\n",
    "test_forecasts = nf.predict() # horizon already = 3\n",
    "print(\"Forecasts generated:\")\n",
    "display(test_forecasts.head())\n",
    "\n",
    "#aligning ground‑truth slice\n",
    "horizon_start = test_forecasts[\"ds\"].min()\n",
    "horizon_end = test_forecasts[\"ds\"].max()\n",
    "\n",
    "truth_slice = test_df[\n",
    "    (test_df[\"ds\"] >= horizon_start) &\n",
    "    (test_df[\"ds\"] <= horizon_end)\n",
    "].copy()\n",
    "\n",
    "#merging truths and predictions\n",
    "pred_col = [c for c in test_forecasts.columns if c != \"ds\" and c != \"unique_id\"][0]\n",
    "eval_df = (\n",
    "    truth_slice.merge(test_forecasts, on=[\"unique_id\", \"ds\"], how=\"left\")\n",
    "               .rename(columns={pred_col: \"y_pred\"})\n",
    ")\n",
    "\n",
    "#derive pollutant label\n",
    "eval_df[\"pollutant\"] = (\n",
    "    eval_df[\"unique_id\"]\n",
    "    .str.extract(r\"_(o3|co|so2|no2)_mean$\")[0]\n",
    "    .str.upper()\n",
    ")\n",
    "\n",
    "# Inverse‑scale to physical ppb \n",
    "eval_df[\"y_ppb\"] = scaler.inverse_transform(eval_df[[\"y\"]])\n",
    "eval_df[\"y_pred_ppb\"] = scaler.inverse_transform(eval_df[[\"y_pred\"]])\n",
    "\n",
    "#computiing the metrics\n",
    "metrics = {}\n",
    "for pol, grp in eval_df.groupby(\"pollutant\"):\n",
    "    metrics[pol] = dict(\n",
    "        RMSE_scaled = math.sqrt(mean_squared_error(grp[\"y\"], grp[\"y_pred\"])),\n",
    "        MAE_scaled = mean_absolute_error(grp[\"y\"], grp[\"y_pred\"]),\n",
    "        R2_scaled = r2_score(grp[\"y\"], grp[\"y_pred\"]),\n",
    "        RMSE_ppb = math.sqrt(mean_squared_error(grp[\"y_ppb\"], grp[\"y_pred_ppb\"])),\n",
    "        MAE_ppb = mean_absolute_error(grp[\"y_ppb\"], grp[\"y_pred_ppb\"]),\n",
    "        R2_ppb = r2_score(grp[\"y_ppb\"], grp[\"y_pred_ppb\"]),\n",
    "    )\n",
    "\n",
    "print(\"Test metrics\")\n",
    "print(\"```\")\n",
    "print(json.dumps(metrics, indent=2))\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a53a7b",
   "metadata": {},
   "source": [
    "# 4. Qualitative plot for one example series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26639ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting one series and inverse‑transform \n",
    "sample_id = eval_df['unique_id'].iloc[0] # selecting first for submission\n",
    "hist = df_nf[df_nf['unique_id'] == sample_id].copy()\n",
    "hist['y_ppb'] = scaler.inverse_transform(hist[['y']])\n",
    "\n",
    "fut  = eval_df[eval_df['unique_id'] == sample_id]\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(hist['ds'], hist['y_ppb'], label='History (ppb)')\n",
    "plt.scatter(fut['ds'], fut['y_ppb'], label='Truth', marker='o')\n",
    "plt.scatter(fut['ds'], fut['y_pred_ppb'], label='Pred', marker='x')\n",
    "plt.title(f'Informer 3‑day forecast – {sample_id}')\n",
    "plt.ylabel('Concentration (ppb)')\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6af86",
   "metadata": {},
   "source": [
    "# 5. Persist artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert nested dict → flat DataFrame\n",
    "metrics_df = (\n",
    "    pd.DataFrame(metrics).T\n",
    "    .reset_index()\n",
    "    .rename(columns={'index':'Pollutant'})\n",
    "    .round(4)\n",
    ")\n",
    "Path('results').mkdir(exist_ok=True)\n",
    "metrics_df.to_csv('results/test_metrics_informer.csv', index=False)\n",
    "import joblib, json\n",
    "joblib.dump({'scaler': scaler}, 'results/scaler.joblib')\n",
    "with open('results/config.json', 'w') as fp: json.dump(CONFIG, fp, indent=2)\n",
    "\n",
    "print('Artefacts saved in ./results/')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "612_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
